# Social Media & Podcast Content: The Friction That Was Protecting You

---
status: draft
created_date: 2026-02-27
topic: AI Hyper-Achievers Burnout
platform: social + podcast
source_article: article-ai-hyperachievers-burnout-no-limits-2026-02-27.md
published: false
---

## Twitter/X Thread

### PLATFORM: Twitter/X Thread (11 tweets)

**Tweet 1 (Hook):**
88% of the highest-productivity AI users report burnout.

Not the skeptics. Not the resisters. The people who embraced AI the most are burning out the fastest.

AI didn't remove the bottleneck. It removed the guardrail.

Here's what nobody saw coming:

(thread)

**Tweet 2 (The Paradox):**
In 1865, economist William Jevons discovered something counterintuitive.

More efficient steam engines didn't reduce coal consumption. They increased it. Dramatically.

Efficiency made coal cheaper per unit, so people found more uses for it. Total consumption exploded.

AI is doing the same thing to human attention.

**Tweet 3 (The Data):**
The numbers tell a clear story:

- 77% of AI users say it added to their workload, not reduced it
- Frequent AI users: 45% higher burnout than infrequent users
- Solo founders: 54% burned out in last 12 months
- Share of solo-founded startups rose from 23.7% to 36.3%

More capacity. More work. Same human ceiling.

**Tweet 4 (The Friction Concept):**
Here's the part that surprised me.

Before AI, there were natural stopping points built into work. Skill gaps forced you to pause. Hiring delays meant you couldn't scale instantly. Debugging took hours that doubled as thinking time.

Those weren't inefficiencies.

They were signals that told ambitious people: "You've done enough for today."

**Tweet 5 (The Removal):**
AI removed those signals.

Can't write code? AI writes it.
Can't design? AI designs it.
Can't draft the strategy doc at midnight? AI drafts it.

Every bottleneck that once forced a break became a prompt away from solving.

The friction was never just friction. Some of it was the mechanism that told you to stop.

**Tweet 6 (The Research):**
University of Toronto researchers put it bluntly in 2026:

"The very friction AI removes may be critical for psychological well-being."

This isn't anti-technology. It's a recognition that constraints serve functions beyond their obvious purpose.

Not all inefficiency is waste.

**Tweet 7 (The Real-World Cost):**
Hieu Pham -- former OpenAI, xAI, Google Brain -- resigned on February 26, 2026 citing burnout.

His statement: "I am burnt out. All the mental health deteriorating that I used to scoff at is real, miserable, scary, and dangerous."

This is someone at the absolute frontier of AI. And even he hit the wall.

**Tweet 8 (The Grind Myth):**
The "grind harder" culture isn't even winning.

Cognition AI requires 80-hour weeks. Their product Devin is among the least adopted AI dev tools on the market.

WHO data: 55+ hours per week = 35% higher stroke risk.

We've confused presence with performance and hours with output.

**Tweet 9 (The Distinction):**
Oliver Burkeman's insight keeps echoing in my head:

The reward for efficiency is more work.

AI amplified this to an unprecedented scale.

AI gave hyper-achievers more capacity, not more time. Those are not the same thing.

Capacity means you CAN do more. Time means you can do it without breaking.

**Tweet 10 (The Voluntary Trap):**
Here's what makes this genuinely tricky.

UC Berkeley researchers (HBR, Feb 2026) found that the work intensification was voluntary. Nobody forced these people to work more.

AI gave them the ability to do more, and ambitious people did what ambitious people do. They filled every gap.

The trap is invisible because you're the one setting it.

**Tweet 11 (The Question):**
Everyone's asking: "How do I use AI to be more productive?"

Almost nobody's asking: "What was that friction protecting me from?"

The bottleneck AI removed might have been the only thing standing between you and burnout.

What's a constraint you've lost that you now realize was serving you?

---

ENGAGEMENT STRATEGY:
- Best posting time: Tuesday-Thursday, 8-10 AM (catches knowledge workers, founders, and tech professionals during morning scroll)
- Expected engagement type: High quote tweets from founders and tech workers sharing personal burnout stories, saves for the Jevons Paradox framework, replies debating whether friction was genuinely protective
- Follow-up content idea: Thread on "How to build artificial friction back into AI-augmented workflows" or "The 3 constraints high-performers are deliberately keeping"

---

## Standalone Twitter/X Posts

### PLATFORM: Twitter/X Single Post (Standalone 1 - The Data Point)

88% of the highest-productivity AI users report burnout.

Not 88% of all workers. Not 88% of reluctant adopters.

88% of the people who embraced AI the most.

Efficiency didn't save them time. It removed the signal that told them to stop.

The Jevons Paradox applied to human attention: make work more efficient, get more total work.

---

ENGAGEMENT STRATEGY:
- Best posting time: Wednesday morning, 8-9 AM
- Expected engagement type: Quote tweets from people recognizing themselves in the data, debate about causation vs. correlation
- Follow-up content idea: Thread breaking down the Jevons Paradox with historical examples applied to personal productivity

---

### PLATFORM: Twitter/X Single Post (Standalone 2 - The Reframe)

AI didn't give you more time.

It gave you more capacity.

Those are fundamentally different things.

More time means you can do more without breaking.
More capacity means you can break faster.

The people burning out hardest right now aren't the AI skeptics. They're the AI power users. That distinction matters.

---

ENGAGEMENT STRATEGY:
- Best posting time: Tuesday or Thursday, early morning
- Expected engagement type: Saves and shares from founders and tech leads, engagement from productivity and mental health communities
- Follow-up content idea: Practical framework for distinguishing capacity from sustainable output

---

### PLATFORM: Twitter/X Single Post (Standalone 3 - The Personal Observation)

Former OpenAI, xAI, and Google Brain researcher Hieu Pham resigned this week citing burnout.

His words: "All the mental health deteriorating that I used to scoff at is real, miserable, scary, and dangerous."

Meanwhile, Cognition AI requires 80-hour weeks. Their product is among the least adopted in its category.

The grind narrative is losing to reality. And the people closest to AI are saying it first.

---

ENGAGEMENT STRATEGY:
- Best posting time: Within 48 hours of Pham's resignation for timeliness (posted Feb 26, 2026)
- Expected engagement type: High engagement from AI/tech community, replies sharing similar experiences, shares from mental health advocates
- Follow-up content idea: Deeper look at which AI companies are burning out teams vs. which are building sustainably

---

## Podcast Q&A Script

### Opening Question (Spoken duration: ~12 seconds)

**QUESTION:**
"I've been sitting with this idea that AI might actually be hurting the people who use it best. That feels counterintuitive. What made you start looking at this?"

---

**ANSWER (Spoken duration: ~3.5 minutes):**

Yeah, it does feel counterintuitive. And honestly, I resisted the idea at first.

Because the story we've all been told is pretty straightforward, right? AI removes tedious work. You get more done in less time. You reclaim your evenings. Everyone wins.

And then I came across this Upwork survey that just... stopped me cold. 88% of the highest-productivity AI users reported burnout symptoms. Not the people who were struggling with AI. Not the ones who couldn't figure out the tools. The power users. The ones doing it "right."

*[pause]*

And then another number: 77% of AI users said AI had actually added to their workload. Not reduced it. Added to it.

So I started pulling on that thread. And what I found was this pattern that kept repeating: the people who embraced AI most enthusiastically were the ones burning out fastest. Frequent AI users showed 45% higher burnout rates than infrequent users.

And, you know, at first I thought maybe this was just a correlation thing. Maybe burned-out people gravitate toward AI looking for relief. But the UC Berkeley research published in Harvard Business Review earlier this year found something really important: the work intensification was voluntary. Nobody was forcing these people to work more hours. Nobody was demanding more output.

*[beat]*

AI gave them the ability to do more. And ambitious people did what ambitious people always do. They filled every gap that opened up.

That's when the Jevons Paradox clicked for me. In 1865, William Jevons discovered that more efficient steam engines didn't reduce coal consumption. They increased it. Because efficiency made coal useful in more contexts. The same thing is happening with human attention and AI. We made work more efficient per unit, and the total volume of work expanded.

The tool that was supposed to give people their time back is actually consuming more of it. And that's not a bug in how people are using AI. I think it might be a feature of how ambition interacts with removed constraints.

---

PRODUCTION NOTES:
- Key emphasis points: "88%," "the power users," "voluntary," "filled every gap"
- Pause moments: After the 88% reveal, after "added to it," before the Jevons Paradox explanation
- Energy level: Starts curious and slightly skeptical of his own thesis, builds to genuine conviction as the evidence accumulates

---

### Follow-up Question 2: The Protective Friction (Spoken duration: ~13 seconds)

**QUESTION:**
"You use this phrase 'protective friction' in your writing. I find it fascinating because we usually think of friction as purely negative. Can you unpack what you mean by that?"

**ANSWER (Spoken duration: ~3 minutes):**

Right, so this is the idea that changed how I think about all of this.

We tend to look at bottlenecks and friction in work as pure waste. Something to be eliminated. And that framing has been the driving force behind every productivity tool ever built, including AI.

But here's what I started noticing. Before AI, if you were a solo founder, there were natural stopping points everywhere. You couldn't code the feature because you needed to learn a new framework first. That took days. You couldn't hire instantly because recruiting takes time. You couldn't debug at 2 AM because the problem genuinely required fresh eyes in the morning.

*[pause]*

Those constraints felt like obstacles. But they were also serving another function. They were the mechanism that told ambitious people, "Okay, you've hit a wall. Time to step back. Sleep. Think. Recover."

And this is the part that really gets me. University of Toronto researchers published findings in 2026 saying -- and I'm paraphrasing but this is close -- "The very friction AI removes may be critical for psychological well-being."

Think about it this way. A speed governor on a car engine isn't making the car worse. It's preventing the car from going faster than the tires and brakes can handle. Remove the governor, and you don't get a better car. You get a faster crash.

*[beat]*

AI removed the speed governor from ambitious people's work. The skill gap that used to take a week to close? AI closes it in minutes. The hiring delay? Build it yourself with AI. The debugging session that forced a coffee break? AI handles it instantly.

Every one of those pauses was a moment where your body and mind got a signal: "This is a natural stopping point." And AI deleted those signals without replacing them with anything.

So you end up with people who can now work at a pace their ambition always wanted but their circumstances never allowed. And it turns out, the circumstances were protecting them.

That's the protective friction. The constraints were never just about inefficiency. Some of them were the guardrails that kept hyper-achievers from running themselves into the ground.

---

PRODUCTION NOTES:
- Key emphasis points: "natural stopping points," "speed governor," "deleted those signals without replacing them," "the circumstances were protecting them"
- Pause moments: After listing the pre-AI constraints, after the University of Toronto quote, after the speed governor analogy
- Energy level: Thoughtful and revelatory. The energy of someone explaining a concept they find genuinely important. The speed governor analogy should land with clarity.

---

### Follow-up Question 3: The Solo Founder Problem (Spoken duration: ~14 seconds)

**QUESTION:**
"You mentioned solo founders specifically. The data shows 54% burned out in the last year, and more people are going solo than ever. Is AI making the solo path more accessible but also more dangerous?"

**ANSWER (Spoken duration: ~3 minutes):**

That's exactly the tension. And I think solo founders are the clearest illustration of this whole dynamic because they don't have the organizational buffers that larger teams provide.

So here's the picture. The share of solo-founded startups rose from 23.7% to 36.3%. AI has made it genuinely possible to do things alone that used to require a team. You can build an MVP, handle customer support, write marketing copy, manage your finances -- all with AI assistance. That's remarkable.

*[pause]*

But 54% of solo founders reported burnout in the last 12 months. More than half.

And when you think about it through the protective friction lens, it makes perfect sense. In a team, you have natural stopping points that are social, not just technical. Your co-founder says, "Hey, let's pick this up tomorrow." Your designer can't deliver the mockup until Friday, so that feature waits. The dev team pushes back on scope.

Those aren't just process constraints. They're human circuit breakers. They force pauses that no individual would impose on themselves.

*[beat]*

The solo founder with AI has none of those. Every constraint that used to force a pause is now solvable. Can't do design? AI can. Can't write the sales email at midnight? AI can. And without a co-founder or team to say "enough," the only limiting factor is the founder's own judgment about when to stop.

And here's the thing about ambitious people. Their judgment about when to stop is terrible. I say that with, you know, full self-awareness. That's kind of the defining feature of ambition -- the inability to feel like you've done enough.

So AI has created this paradox where the path to building a company alone is more accessible than ever, but the psychological cost of walking that path is also higher than ever. Because you've removed all the external constraints that used to compensate for your internal inability to stop.

I don't think the answer is "don't go solo" or "don't use AI." But I think the honest conversation is: if you're going solo with AI, you need to deliberately build back the friction that used to exist naturally. And that requires a kind of self-awareness that most people in the heat of building don't have.

---

PRODUCTION NOTES:
- Key emphasis points: "human circuit breakers," "their judgment about when to stop is terrible," "deliberately build back the friction"
- Pause moments: After the 54% burnout stat, after "the only limiting factor is the founder's own judgment," before the paradox conclusion
- Energy level: Empathetic and direct. Speaking as someone who recognizes this pattern, possibly in themselves. The self-awareness note should feel genuine, not performative.

---

### Follow-up Question 4: The Hieu Pham Story (Spoken duration: ~12 seconds)

**QUESTION:**
"You mentioned Hieu Pham's resignation from the AI world. He literally used to scoff at mental health concerns. What does it mean when the people building AI are the ones breaking?"

**ANSWER (Spoken duration: ~2.5 minutes):**

Yeah, this one hit me. Because it's one thing to see survey data about burnout rates. It's another to see someone at the absolute frontier -- OpenAI, xAI, Google Brain -- publicly say, "I am burnt out. All the mental health deteriorating that I used to scoff at is real, miserable, scary, and dangerous."

That word "scoff." He used to scoff at it. And now he's living it.

*[pause]*

I think there's something really important in that trajectory. Because Pham isn't someone who was struggling with AI. He was building it. He understood it better than almost anyone. And that understanding didn't protect him. If anything, being at the frontier meant he had even fewer external constraints on his work. When you're defining what the technology can do, there's no manual that says "this is where you stop for the day."

And then look at Cognition AI, the company behind Devin. They reportedly require 80-hour weeks. And their product? It's among the least adopted AI developer tools on the market.

*[beat]*

So you have this grinding culture that isn't even producing better outcomes. The WHO data on this is stark: working 55 or more hours per week increases stroke risk by 35% and heart disease risk by 17%. This isn't soft advice about work-life balance. This is mortality data.

The companies that are burning people out the hardest aren't outperforming. The individuals who are grinding the most aren't producing better work. The research on sustained cognitive performance is clear: after a point, more hours produce worse output, not more output.

And I think what Pham's story represents is the leading edge of a much larger wave. The people closest to the technology, the ones with the fewest remaining constraints, are hitting the wall first. The rest of us are a few steps behind.

That should be a signal. Not a cautionary tale about one person, but an early warning about the direction we're all headed if we treat removed friction as an unqualified good.

---

PRODUCTION NOTES:
- Key emphasis points: "scoff," "that understanding didn't protect him," "not even producing better outcomes," "early warning"
- Pause moments: After quoting Pham, after the Cognition AI point, before the mortality data
- Energy level: Somber but not heavy-handed. Respect for Pham's vulnerability. Builds from individual story to systemic insight.

---

### Follow-up Question 5: The Voluntary Trap (Spoken duration: ~14 seconds)

**QUESTION:**
"So here's what I'm wrestling with. You said the work intensification was voluntary. Nobody forced anyone. Doesn't that make this a personal responsibility issue rather than an AI problem? Where do you draw that line?"

**ANSWER (Spoken duration: ~3 minutes):**

That's the hardest question in this whole conversation, and I want to be honest that I don't have a perfectly clean answer.

Because on one level, yes. Nobody held a gun to anyone's head and said "work until you burn out." These are adults making choices. And I think there's something important about not removing personal agency from the equation.

*[pause]*

But here's where I push back on the pure "personal responsibility" framing.

Think about it this way. If you put an open bag of chips in front of someone, and they eat the whole bag, you could say "well, that's a willpower problem." And you'd be technically right. But if you redesign the chips to be more addictive, make the bag impossible to close, remove all the other food options, and then 88% of chip-eaters report health problems... at some point you have to ask whether the environment is doing more work than individual willpower can overcome.

*[beat]*

AI created an environment where every natural stopping point in work has been removed. The friction that used to force breaks is gone. The skill gaps that used to create pauses are gone. The constraints that made ambitious people step back are gone.

And you're asking individuals -- specifically the most ambitious, driven individuals, the ones whose entire identity is built around doing more -- to voluntarily impose limits on themselves in an environment that has removed every external limit.

That's a really hard ask. And the 88% burnout rate among high-productivity users suggests it's an ask most people are failing at.

So I think it's both. Personal responsibility matters. You ultimately have to be the one who decides to close the laptop. But pretending this is purely a willpower issue ignores the fact that the environment has fundamentally changed.

Oliver Burkeman has this insight I keep coming back to: the reward for efficiency is more work. That was true before AI. But AI amplified it to an unprecedented scale. The reward for AI-powered efficiency is an almost infinite expansion of what you could be doing.

And "could" is the most dangerous word for an ambitious person. Because "could" never has a natural endpoint. Before AI, "could" bumped into "but I can't because I don't have the skills, the team, the time." AI removed most of those "buts."

So now ambitious people face an open field with no fences. And we're surprised they're running themselves to exhaustion?

---

PRODUCTION NOTES:
- Key emphasis points: "environment is doing more work than willpower can overcome," "most dangerous word for an ambitious person," "open field with no fences"
- Pause moments: After the chips analogy, after "every external limit," before the Burkeman insight
- Energy level: Genuinely wrestling with the tension. Not preachy. Holds both sides honestly. The chips analogy should be delivered casually, almost conversationally.

---

### Follow-up Question 6: The Capacity vs. Time Distinction (Spoken duration: ~11 seconds)

**QUESTION:**
"You make this distinction between AI giving people more capacity versus more time. That feels like a really important distinction. Can you break that down?"

**ANSWER (Spoken duration: ~2.5 minutes):**

Yeah, I think this might be the single most important reframe in this whole conversation.

So the promise of AI -- the implicit promise, the one in all the marketing -- is: "AI will save you time." And on a task level, that's true. Something that took two hours now takes twenty minutes. You've saved 100 minutes. That's real.

But what happens to those 100 minutes?

*[pause]*

If you're an ambitious person -- and the people adopting AI most aggressively tend to be -- those 100 minutes don't become free time. They become capacity. They become space to start the next project, answer the next email, build the next feature, write the next document.

AI didn't give you a shorter workday. It gave you the ability to fit more work into the same day. That's capacity, not time.

And here's why the distinction matters. Time savings have a natural ceiling. You save two hours, you have two extra hours. You can choose to rest, to think, to do nothing.

Capacity has no ceiling. Because capacity just means "you can do more." And for someone whose internal compass always points toward "more," capacity is like handing an unlimited credit card to someone with a shopping addiction. The constraint was the only thing preventing overuse.

*[beat]*

The data bears this out perfectly. 77% of AI users say it increased their workload. Not because their boss demanded more. Because they could do more, so they did more. The capacity expanded to fill -- and then exceed -- the available human energy.

This is the Jevons Paradox applied to human attention. More efficient use of attention per task leads to more total attention spent on work. The efficiency gain gets consumed by expanded ambition.

So when someone tells me "AI is saving me so much time," I've started asking: "Are you working fewer hours?" And the answer, almost universally, is no. They're working the same hours or more. They're just doing more within those hours.

That's not a time savings. That's an intensity increase. And intensity without recovery is the textbook definition of burnout.

---

PRODUCTION NOTES:
- Key emphasis points: "capacity, not time," "no ceiling," "unlimited credit card," "intensity increase"
- Pause moments: After "what happens to those 100 minutes?", after "preventing overuse," before the final reframe
- Energy level: Clear and precise, almost like teaching. This is the conceptual core, so delivery should be measured and deliberate. The credit card analogy should land with a slight beat after it.

---

### Follow-up Question 7: The Counterargument (Spoken duration: ~13 seconds)

**QUESTION:**
"I can already hear people saying, 'But this is just the adjustment period. People will learn to set boundaries with AI like they did with email and smartphones.' What do you think about that?"

**ANSWER (Spoken duration: ~2.5 minutes):**

I mean, have we learned to set boundaries with email and smartphones?

*[pause, let it land]*

I'm being a little provocative there, but I think the point stands. We've had smartphones for almost two decades. Average screen time is still going up. Email was supposed to make communication more efficient, and most knowledge workers now spend 28% of their workweek on email. We didn't set boundaries. We adapted to a new, higher baseline of always-on work.

So the track record on "we'll adjust" isn't great.

*[beat]*

But look, I think there's a more nuanced answer too. Some people will absolutely figure this out. Some already have. They'll build deliberate constraints back into their workflows. They'll set hard stops. They'll use AI selectively rather than for everything.

But those people tend to be the ones who've already burned out once and learned the lesson the hard way. They're building boundaries from scar tissue, not from foresight.

The question is whether we can get ahead of this as a broader culture. And history suggests... probably not quickly. We tend to adopt technologies fast and figure out the human costs slowly. Social media is the most recent example. We're still, a decade and a half in, trying to figure out what it's doing to mental health.

What concerns me most is the speed differential. AI is being adopted faster than any previous technology. And the human capacity to develop new norms and boundaries around technology hasn't gotten any faster.

So the adjustment will come. But my worry is that a lot of people will burn out before it does. Especially the most ambitious ones, the ones least likely to pump the brakes voluntarily.

And I think acknowledging that is more useful than pretending it's just a matter of "setting better boundaries." The friction was doing work that individual willpower now has to replace. And willpower, as we know from decades of psychology research, is a depletable resource. We're asking people to use willpower to constrain themselves in an environment specifically designed to remove constraints.

That's a structural mismatch, not just a personal development challenge.

---

PRODUCTION NOTES:
- Key emphasis points: "have we learned to set boundaries with email?", "scar tissue, not foresight," "structural mismatch"
- Pause moments: After the opening retort, after the screen time data, before the final structural mismatch point
- Energy level: Slightly provocative at the opening, then genuinely thoughtful. Not dismissive of the counterargument -- takes it seriously and finds the nuance.

---

### Follow-up Question 8: The Closing Reflection (Spoken duration: ~10 seconds)

**QUESTION:**
"After sitting with all of this -- the data, the stories, the patterns -- what's the one thing you'd want people to take away?"

**ANSWER (Spoken duration: ~2 minutes):**

I think it's this: not all friction is waste.

That sounds simple, but I think it cuts against the deepest assumption of the productivity era we've been living in. We've spent decades being told that friction is the enemy. That efficiency is always good. That removing obstacles is always progress.

And AI is the ultimate expression of that philosophy. It removes friction at a scale we've never seen before.

*[pause]*

But some of that friction was protecting you. Some of those bottlenecks were the only thing standing between your ambition and your breaking point. And removing them didn't free you. It exposed you.

The data is pretty clear: 88% of the people who used AI most productively are burning out. The people at the frontier of AI development are publicly breaking down. The hours-grinding culture isn't producing better products.

*[beat]*

So the question I'd want people to sit with is not "How do I use AI to do more?" It's "What was that friction doing for me, and what replaces it now that it's gone?"

Because AI gave us more capacity. It didn't give us more resilience. It didn't give us more clarity about what actually matters. It didn't give us a better sense of when to stop.

Those things have to come from somewhere. And if they used to come from external constraints that no longer exist, then you have to build them deliberately. That's not a productivity hack. That's a survival skill.

The friction was protecting you. Now you have to protect yourself.

---

PRODUCTION NOTES:
- Key emphasis points: "not all friction is waste," "exposed you," "survival skill," "the friction was protecting you"
- Pause moments: After "progress," after "breaking point," before the final two lines
- Energy level: Reflective and grounded. The energy of someone delivering a conclusion they believe deeply. Should end with weight, not drama. The last line should feel like a quiet conviction, not a rallying cry.

---

## Content Metrics & Testing Notes

### A/B Testing Suggestions:
- Test thread hook: "88% burnout stat" lead vs. Jevons Paradox historical lead
- Compare standalone tweet performance: data-driven (88% stat) vs. reframe (capacity vs. time) vs. timely (Pham resignation)
- Test thread length: full 11 tweets vs. condensed 7-tweet version cutting tweets 8 and 10

### Repurposing Opportunities:
- The capacity vs. time distinction works as a standalone LinkedIn post
- Jevons Paradox applied to human attention could be a visual graphic or short explainer video
- Hieu Pham story + Cognition AI data works as a timely standalone LinkedIn micro-post
- The "speed governor" analogy from the podcast can be extracted as a standalone social post
- Podcast Q&A on solo founders can be trimmed into a 90-second video script
- The chips analogy from Q&A 5 works as a standalone tweet

### Voice Verification:
- [x] Counterintuitive hook (the people using AI best are burning out most)
- [x] Evidence-based throughout (Upwork data, UC Berkeley/HBR, WHO, U of Toronto)
- [x] Specific numbers (88%, 77%, 45%, 54%, 35%, 55+ hours)
- [x] Personal curiosity framing ("this surprised me," "I resisted the idea at first")
- [x] Not preachy (exploring a paradox, admitting uncertainty)
- [x] Challenges without attacking (questions the narrative, doesn't blame AI or users)
- [x] Real stories (Hieu Pham, Cognition AI, Jevons historical parallel)
- [x] Engagement prompts at thread close
- [x] Podcast Q&A: curious, direct but warm, explores nuance
- [x] Podcast Q&A: challenges gently ("have we learned boundaries with email?")
- [x] Podcast Q&A: holds uncertainty honestly ("I don't have a perfectly clean answer")
- [x] No motivational fluff -- every point grounded in data or concrete example
- [x] Capacity vs. time distinction as original conceptual contribution
